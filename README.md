# Visual Search

This project provides a complete solution for visual search for images, videos and text. The backend makes use of Amazon Sagemaker and other AWS services to provide a scalable and cost-effective API that can be used to search for similar images and videos, and apply contextual filters based on text as well as hard filters. The frontend is a simple web application that allows users to upload images and videos, and search for similar images and videos based on the uploaded content.

## How it Works

We make use of `Amazon SageMaker` and `Amazon OpenSearch Service` to implement unified text and image search with a `CLIP` model. `Embedding-based retrieval (EBR)` is used in search and recommendation systems to find similar items from a vector database, leveraging semantic concepts instead of just keywords. 

Contrastive Language-Image Pre-Training (CLIP) encodes images and text into the same space for comparison using cosine similarity. Encoded product images or descriptions are stored in a vector database, which supports both image and text searches. 

### Proof of Concept

https://github.com/Harsh1s/Visual-Search-API/assets/96648934/271a7f42-d636-46b6-ac73-03eacf68ff40

As seen in the video, you can use the [web application](https://viz-search.netlify.app/) to currently:
- Upload an image and fetch a limited number of similar images (which would be actually be associated with products in production).
- You can even upload a video clip, and our backend would
  - Extract frames from the video clip at a secondly interval.
  - Fetch the common distinct frames from the video clip.
  - Utilise the same backend architecture to query for the best matching products.
  
- The backend model also assoiates semantic text with the images, so you can search for products based on text as well. These text queries can be either used for soft filtering or hard filtering, thus allowing them to be applicable in any settings. 

## Solution Architecture

- `Amazon S3` is used to store the raw product description text, images and image embedding generated by the SageMaker Batch Transform jobs.
- A `SageMaker` Model is created from a pretrained CLIP model for batch and real-time inference.
- SageMaker batch transform jobs are used to generate embeddings of product images.
- SageMaker serverless inference is used to encode query image and text into embeddings in real-time.
- `OpenSearch Service` is the search engine to perform KNN-based search.
- A query function is used to orchestrate encoding query and perform KNN-based search.
- `SageMaker Studio Notebooks` will be used as IDE to develop the solution.

## Dataset

The `Amazon Berkeley Objects Dataset` is used in the implementation. The dataset is a collection of `147,702` product listings with multilingual metadata and `398,212` unique catalogue images. We will only make use of the item images and item names in US English. For demo purposes and costs consideration, we only trained the same to use ~1,600 products.

