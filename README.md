# Visual Search

This project provides a complete solution for visual search for images, videos and text. The backend makes use of Amazon Sagemaker and other AWS services to provide a scalable and cost-effective API that can be used to search for similar images and videos, and apply contextual filters based on text as well as hard filters. The frontend is a simple web application that allows users to upload images and videos, and search for similar images and videos based on the uploaded content.

## Use Amazon SageMaker and Amazon OpenSearch Service to implement unified text and image search with a CLIP model

Embedding-based retrieval (EBR) is used in search and recommendation systems to find similar items from a vector database, leveraging semantic concepts instead of just keywords.

This repository builds an ML-powered search engine prototype using Amazon OpenSearch Service and Amazon SageMaker. It supports product retrieval and recommendation based on text or image queries.

Contrastive Language-Image Pre-Training (CLIP) encodes images and text into the same space for comparison using cosine similarity. Encoded product images or descriptions are stored in a vector database, which supports both image and text searches.

Customers can search by taking a picture or describing a product in text. Results are sorted by similarity scores, with the top K products being the most relevant.

## Solution Overview

- Amazon S3 is used to store the raw product description text & images and image embedding generated by the SageMaker Batch Transform jobs.
- A SageMaker Model is created from a pretrained CLIP model for batch and real-time inference.
- SageMaker Batch Transform job is used to generate embeddings of product images.
- SageMaker Serverless inference is used to encode query image and text into embeddings in real-time.
- OpenSearch Service is the search engine to perform KNN-based search.
- A query function is used to orchestrate encoding query and perform KNN-based search.
- SageMaker Studio Notebooks(not in the diagram) will be used as IDE to develop the solution.

## Dataset

The Amazon Berkeley Objects Dataset is used in the implementation. The dataset is a collection of 147,702 product listings with multilingual metadata and 398,212 unique catalogue images. We will only make use of the item images and item names in US English. For demo purposes we are going to use ~1,600 products.
